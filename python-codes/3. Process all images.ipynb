{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1543d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = [\"Camden\", \"Greenwich\", \"Hackney\", \n",
    "            \"Islington\", \"Lambeth\", \"Lewisham\",\n",
    "            \"Southwark\", \"Wandsworth\", \"Westminster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf71c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = [\"City_of_London\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "803ff973",
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = [\"Hammersmith_and_Fulham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bd462b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = [\"Tower_Hamlets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e35175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'all_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ecf1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, TFSegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796706de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792cf55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "C:\\Users\\win10\\.conda\\envs\\yyjj125\\lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Conv._jit_compiled_convolution_op at 0x000002479B7990D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Conv._jit_compiled_convolution_op at 0x000002479B7ABA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFSegformerForSemanticSegmentation.\n",
      "\n",
      "All the layers of TFSegformerForSemanticSegmentation were initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFSegformerForSemanticSegmentation for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# SegFormer\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = TFSegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35615c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\win10/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "Using cache found in C:\\Users\\win10/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_type = \"DPT_Large\" \n",
    "\n",
    "# MiDas\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# Load the appropriate transformation function based on the model type\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3bffddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "width=640\n",
    "height=640\n",
    "tree = 4\n",
    "grass = 9\n",
    "shrub=17\n",
    "total=width*height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d68e6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate borough\n",
    "for borough in boroughs:\n",
    "    image_dir = borough\n",
    "    results = []\n",
    "    if not os.path.isdir(image_dir):\n",
    "        print(f'no: {image_dir}')\n",
    "        continue\n",
    "    image_files = os.listdir(image_dir)\n",
    "    for image_file in image_files:.\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        img_np = np.array(img)\n",
    "        inputs = image_processor(images=img, return_tensors=\"tf\")\n",
    "        outputs = model(**inputs, training=False)\n",
    "        input_batch = transform(img_np).to(device) \n",
    "        with torch.no_grad():\n",
    "            prediction = midas(input_batch)\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img.size,\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            ).squeeze()      \n",
    "        depth_output = prediction.cpu().numpy()\n",
    "        label_map = tf.argmax(outputs.logits, axis=1).numpy()\n",
    "        label_map_first = label_map[0]\n",
    "        label_map_resized = cv2.resize(label_map_first, (prediction.shape[1], prediction.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        for label, label_name in zip([tree, grass, shrub], ['tree', 'grass', 'shrub']):\n",
    "            label_map_binary = (label_map_resized == label)\n",
    "            num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(label_map_binary.astype('uint8'), connectivity=8)\n",
    "            instance_counter = 0\n",
    "            for i in range(1, num_labels):\n",
    "                instance_pixels = stats[i, -1]\n",
    "                if instance_pixels > 1250:\n",
    "                    instance_percentage = (instance_pixels / total) * 100\n",
    "                    instance_depth = depth_output[labels == i].mean()\n",
    "                    instance_counter += 1  # Increment instance counter\n",
    "                    filename_parts = image_file.split('_')\n",
    "                    point_id = int(filename_parts[3])\n",
    "                    heading = int(filename_parts[4].split('.')[0])\n",
    "                    results.append({\n",
    "                        'PointID': point_id,\n",
    "                        'heading': heading,\n",
    "                        'category': label_name,\n",
    "                        'object': instance_counter,\n",
    "                        'percentage of green': instance_percentage,\n",
    "                        'average depth': instance_depth\n",
    "                    })\n",
    "\n",
    "    # make df for results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    # save to CSV\n",
    "    df_results.to_csv(os.path.join(results_dir, f\"{borough}_original.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I acknowledge the use of ChatGPT 4.0 (Open AI, https://chat.openai.com) to debug."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
